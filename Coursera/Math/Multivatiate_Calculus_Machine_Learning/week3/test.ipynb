{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598749790614",
   "display_name": "Python 3.7.5 64-bit ('tf2_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed forward\n",
    "\n",
    "In the following cell, we will define functions to set up our neural network.\n",
    "Namely an activation function, $\\sigma(z)$, it's derivative, $\\sigma'(z)$, a function to initialise weights and biases, and a function that calculates each activation of the network using feed-forward.\n",
    "\n",
    "Recall the feed-forward equations,\n",
    "$$ \\mathbf{a}^{(n)} = \\sigma(\\mathbf{z}^{(n)}) $$\n",
    "$$ \\mathbf{z}^{(n)} = \\mathbf{W}^{(n)}\\mathbf{a}^{(n-1)} + \\mathbf{b}^{(n)} $$\n",
    "\n",
    "In this worksheet we will use the *logistic function* as our activation function, rather than the more familiar $\\tanh$.\n",
    "$$ \\sigma(\\mathbf{z}) = \\frac{1}{1 + \\exp(-\\mathbf{z})} $$\n",
    "\n",
    "There is no need to edit the following cells.\n",
    "They do not form part of the assessment.\n",
    "You may wish to study how it works though.\n",
    "\n",
    "**Run the following cells before continuing.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PACKAGE\n",
    "# First load the worksheet dependencies.\n",
    "# Here is the activation function and its derivative.\n",
    "sigma = lambda z : 1 / (1 + np.exp(-z))\n",
    "d_sigma = lambda z : np.cosh(z/2)**(-2) / 4\n",
    "\n",
    "def reset_network(n1=6, n2=7):\n",
    "    \"\"\"\n",
    "    This function initializes the network with it's structure\n",
    "    \"\"\"\n",
    "    global W1, W2, W3, b1, b2, b3 \n",
    "    W1 = random.rand(n1, 1)/2\n",
    "    W2 = random.rand(n2, n1)/2\n",
    "    W3 = random.rand(2, n2)/2\n",
    "    b1 = random.rand(n1, 1)/2\n",
    "    b2 = random.rand(n2, 1)/2\n",
    "    b3 = random.rand(2, 1)/2\n",
    "\n",
    "def network_function(a0):\n",
    "    \"\"\"\n",
    "    This function feeds forward each activation to the next layer.\n",
    "    It returns all weighted sums.\n",
    "    \"\"\"\n",
    "    z1 = W1@a0 + b1 \n",
    "    a1 = sigma(z1)\n",
    "    z2 = W2@a1 + b2 \n",
    "    a2 = sigma(z2)\n",
    "    z3 = W3@a2 + b3 \n",
    "    a3 = sigma(z3)\n",
    "    return a0, a1, z1, a2, z2, a3, z3 \n",
    "\n",
    "def cost1(x, y):\n",
    "    \"\"\"\n",
    "    This function calcute cost function.\n",
    "    \"\"\"\n",
    "    return np.linalg.norm(network_function(x)[-1] - y)**2 / x.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###             Gradient function\n",
    "\n",
    "def J_W3(x, y):\n",
    "    \"\"\"\n",
    "    This function computer Jacobian for the third layer weights.\n",
    "    \"\"\"\n",
    "    a0, a1, z1, a2, z2, a3, z3 = network_function(x)\n",
    "    #       dC/da3\n",
    "    J = 2*(a3 - y)\n",
    "    #       Calcutated by the derivative of sigma at z3\n",
    "    J = J*d_sigma(z3)\n",
    "    #       Take the dot product with the final partial derivative \n",
    "    # divide by the number of training examples -> the average over all\n",
    "    J = J @ a2.T / x.size\n",
    "    # Finally return the result out of the function.\n",
    "    return J\n",
    "\n",
    "def J_b3 (x, y) :\n",
    "    # As last time, we'll first set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # Next you should implement the first two partial derivatives of the Jacobian.\n",
    "    # ===COPY TWO LINES FROM THE PREVIOUS FUNCTION TO SET UP THE FIRST TWO JACOBIAN TERMS===\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    # For the final line, we don't need to multiply by dz3/db3, because that is multiplying by 1.\n",
    "    # We still need to sum over all training examples however.\n",
    "    # There is no need to edit this line.\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###             Gradient function\n",
    "\n",
    "def J_W2(x, y):\n",
    "    \"\"\"\n",
    "    This function computer Jacobian for the third layer weights.\n",
    "    \"\"\"\n",
    "    a0, a1, z1, a2, z2, a3, z3 = network_function(x)\n",
    "    #       dC/da3\n",
    "    J = 2*(a3 - y)\n",
    "    #       da3/da2\n",
    "    J = J*d_sigma(z3)@\n",
    "    #       Calcutated by the derivative of sigma at z3\n",
    "    J = J*d_sigma(z3)\n",
    "    #       Take the dot product with the final partial derivative \n",
    "    # divide by the number of training examples -> the average over all\n",
    "    J = J @ a2.T / x.size\n",
    "    # Finally return the result out of the function.\n",
    "    return J\n",
    "\n",
    "def J_b3 (x, y) :\n",
    "    # As last time, we'll first set up the activations.\n",
    "    a0, z1, a1, z2, a2, z3, a3 = network_function(x)\n",
    "    # Next you should implement the first two partial derivatives of the Jacobian.\n",
    "    # ===COPY TWO LINES FROM THE PREVIOUS FUNCTION TO SET UP THE FIRST TWO JACOBIAN TERMS===\n",
    "    J = 2 * (a3 - y)\n",
    "    J = J * d_sigma(z3)\n",
    "    # For the final line, we don't need to multiply by dz3/db3, because that is multiplying by 1.\n",
    "    # We still need to sum over all training examples however.\n",
    "    # There is no need to edit this line.\n",
    "    J = np.sum(J, axis=1, keepdims=True) / x.size\n",
    "    return J"
   ]
  }
 ]
}